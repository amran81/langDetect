{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c79f418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import openpyxl\n",
    "\n",
    "\n",
    "#Google\n",
    "from langdetect import detect, detect_langs,DetectorFactory\n",
    "# DetectorFactory.seed = 1  # Optional, for consistency\n",
    "#Spacy\n",
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector\n",
    "from spacy.language import Language\n",
    "#fasttext\n",
    "import fasttext\n",
    "#Roberta\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3d77b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "C:\\Python311\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.7.1) was trained with spaCy v3.7.2 and may not be 100% compatible with the current version (3.8.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "#fastext model\n",
    "# Load the FastText pre-trained language identification model\n",
    "model_path_fasttext = 'C:/Python311/fastText/lid.176.bin'\n",
    "model_fasttext = fasttext.load_model(model_path_fasttext)\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Load the model and tokenizer\n",
    "model_roberta = \"papluca/xlm-roberta-base-language-detection\"\n",
    "tokenizer_roberta = AutoTokenizer.from_pretrained(model_roberta)\n",
    "modelRoberta = AutoModelForSequenceClassification.from_pretrained(model_roberta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bd50276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the filtered DataFrame: 2621\n"
     ]
    }
   ],
   "source": [
    "def save_to_excel(df, path, sheet_name):\n",
    "    with pd.ExcelWriter(path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "def classify_language_type_roberta(text):\n",
    "    sentences = text.split('.')\n",
    "    detected_languages = set()\n",
    "    #num_elements=0\n",
    "    detected_mix = False  # Flag to indicate if 'mix' should be returned\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence:\n",
    "            # Tokenize and classify the sentence using the model\n",
    "            inputs = tokenizer_roberta(sentence, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = modelRoberta(**inputs).logits\n",
    "\n",
    "            preds = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            # Map raw predictions to languages\n",
    "            id2lang = modelRoberta.config.id2label\n",
    "            vals, idxs = torch.max(preds, dim=1)\n",
    "\n",
    "            # Get the predicted language and add it to detected_languages\n",
    "            for k in idxs:\n",
    "                detected_languages.add(id2lang[k.item()])\n",
    "                #num_elements=num_elements+1\n",
    "                #print(detected_languages)\n",
    "\n",
    "#     print(f\"Detected languages: {detected_languages}\")  # Debugging information\n",
    "    if len(detected_languages) > 1:\n",
    "        return 'mix'\n",
    "    elif len(detected_languages) == 1:\n",
    "        return detected_languages.pop()  # Return the single language in the set\n",
    "    else:\n",
    "        return 'unknown'  # Handle the case where no language is detected\n",
    "    \n",
    "def detect_language_nltk(text):\n",
    "    if not text:\n",
    "        return \"unknown\"#, 0.0\n",
    "    try:\n",
    "        langs = detect_langs(text)\n",
    "        if len(langs) > 1:\n",
    "            return \"mix\"#, 1.0  # Consider it mixed if multiple languages are detected\n",
    "        else:\n",
    "            lang = langs[0].lang\n",
    "            return lang#, langs[0].prob\n",
    "    except:\n",
    "        return \"unknown\"#, 0.0\n",
    "    \n",
    "\n",
    "    \n",
    "# Function to create the language detector component\n",
    "def create_language_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "# Try to register the language detector with SpaCy\n",
    "try:\n",
    "    Language.factory(\"language_detector\", func=create_language_detector)\n",
    "    if \"language_detector\" not in nlp.pipe_names:\n",
    "        nlp.add_pipe('language_detector', last=True)\n",
    "except ValueError:\n",
    "    pass  # The component is already registered\n",
    "\n",
    "\n",
    "def detect_language_spacy(text):\n",
    "    if not text:\n",
    "        return \"unknown\"\n",
    "    \n",
    "    # Split text into sentences or paragraphs\n",
    "    sentences = text.split('. ')\n",
    "    detected_languages = set()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        lang = doc._.language['language']\n",
    "        detected_languages.add(lang)\n",
    "    \n",
    "    # Aggregate results\n",
    "    if len(detected_languages) > 1:\n",
    "        return 'mix'\n",
    "    elif len(detected_languages) == 1:\n",
    "        return detected_languages.pop()  # Return the single language in the set\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "\n",
    "def detect_language_fasttext(text):\n",
    "    if not text:\n",
    "        return \"unknown\"#, 0.0\n",
    "    \n",
    "    predictions = model_fasttext.predict([text], k=2)  # Get top 2 predictions\n",
    "    top_label = predictions[0][0][0].replace(\"__label__\", \"\")\n",
    "    top_probability = predictions[1][0][0]\n",
    "    \n",
    "    # Check if top prediction has high confidence and no other prediction with significant probability\n",
    "    if top_probability > 0.95 and len(predictions[0]) == 1:\n",
    "        return top_label #, top_probability\n",
    "    else:\n",
    "        return \"mix\"#, 1.0\n",
    "    \n",
    "# Load the Excel file\n",
    "file_name = \"SubjectAppsDataset_exp03.xlsx\"\n",
    "file_path = file_name\n",
    "# Read the Excel file\n",
    "xls = pd.ExcelFile(file_path)\n",
    "# Read the specific sheet you want to analyze\n",
    "sheet_name = \"Sheet1\"\n",
    "filtered_df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "# Count the number of records in the filtered DataFrame\n",
    "record_count = len(filtered_df)\n",
    "print(f\"Number of records in the filtered DataFrame: {record_count}\")\n",
    "\n",
    "# Now you can iterate over the filtered DataFrame\n",
    "for index, row in filtered_df.iterrows():    \n",
    "    review = row['Normalization']\n",
    "    \n",
    "    # Handle NaN values or empty strings\n",
    "    if isinstance(review, float) and pd.isna(review):\n",
    "        review = \"\"\n",
    "    \n",
    "    if review == \"\":\n",
    "        max_label = \"unknown\"\n",
    "        max_probability = 0.0\n",
    "    else:\n",
    "        nltk_lang = detect_language_nltk(review)\n",
    "        spacy_lang = detect_language_spacy(review)\n",
    "        fasttext_lang = detect_language_fasttext(review)\n",
    "        roberta_lang = classify_language_type_roberta(review)\n",
    "        \n",
    "    # Update the filtered DataFrame\n",
    "    filtered_df.at[index, 'Exp_Language_Detection[NLTK]'] = nltk_lang\n",
    "    filtered_df.at[index, 'Exp_Language_Detection[Spacy]'] = spacy_lang\n",
    "    filtered_df.at[index, 'Exp_Language_Detection[FastText]'] = fasttext_lang\n",
    "    filtered_df.at[index, 'Exp_Language_Detection[Roberta]'] = roberta_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "558d3481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ File saved to 'SubjectAppsDataset_exp03.xlsx' in sheet 'baseline'.\n"
     ]
    }
   ],
   "source": [
    "sheet_name=\"baseline\"\n",
    "save_to_excel(filtered_df, file_path, sheet_name)\n",
    "# --- Final Status ---\n",
    "print(f\"\\n✅ File saved to '{file_path}' in sheet '{sheet_name}'.\")\n",
    "xls.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "16c80614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n"
     ]
    }
   ],
   "source": [
    "sentence=\"\"\"\n",
    "this app contain bug it makes my fullscreen navigation system unusable whenever i use this app i cannot exit or use back gesture and i will stick in the app or sometimes if the app runs on background will also disable back and exit gesture it almost like the system detect myjpj app as a launcher please fix la bebal menyusahkan orang je la dah la lesen physical tak bagi bengap betul tak pasal2 aku kena restart fon bagai sebab bug bodo ni tiba2 act sebagai launcher pulak tak nyaman la macam ni.\n",
    "\"\"\"\n",
    "\n",
    "nltkPreditc=detect_language_nltk(sentence)\n",
    "print(nltkPreditc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c2760c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
